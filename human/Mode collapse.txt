In machine learning, mode collapse is a failure mode observed in generative models, originally noted in Generative Adversarial Networks (GANs). It occurs when the model produces outputs that are less diverse than expected, effectively "collapsing" to generate only a few modes of the data distribution while ignoring others. This phenomenon undermines the goal of generative models to capture the full diversity of the training data.
There are typically two times at which a model can collapse: either during training or during post-training finetuning.
Mode collapse reduces the utility of generative models in applications, such as in

image synthesis (repetitive or near-identical images);
data augmentation (limited diversity in synthetic data);
scientific simulations (failure to explore all plausible scenarios).